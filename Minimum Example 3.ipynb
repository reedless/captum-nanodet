{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967d938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from nanodet.util import cfg, load_config, Logger\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052742c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'assets/nanodet-300/nanodet-300.pth_train_config.yml'\n",
    "model_path = 'assets/nanodet-300/nanodet-300.pth'\n",
    "image_path = 'dataset/night/20201201_000505.jpg'\n",
    "\n",
    "load_config(cfg, config_path)\n",
    "logger = Logger(-1, use_tensorboard=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2c7028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size is  1.0x\n",
      "init weights...\n",
      "=> loading pretrained model https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\n",
      "Finish initialize NanoDet Head.\n"
     ]
    }
   ],
   "source": [
    "from nanodet.model.arch import build_model\n",
    "from nanodet.util import Logger, cfg, load_config, load_model_weight\n",
    "from nanodet.data.transform import Pipeline\n",
    "from nanodet.data.collate import naive_collate\n",
    "from nanodet.data.batch_process import stack_batch_img\n",
    "import numpy as np\n",
    "\n",
    "class WrapperModel(torch.nn.Module):\n",
    "    def __init__(self, cfg, model_path, logger, device=\"cuda:0\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        \n",
    "        self.num_classes = cfg['model']['arch']['head']['num_classes']\n",
    "        self.reg_max = cfg['model']['arch']['head']['reg_max']\n",
    "        model = build_model(cfg.model)\n",
    "        ckpt = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "        load_model_weight(model, ckpt, logger)\n",
    "        self.model = model.to(device).eval()\n",
    "\n",
    "    def forward(self, tensor_img):\n",
    "        if len(tensor_img.shape) == 4:\n",
    "\n",
    "            preds = self.model(tensor_img)\n",
    "            cls_scores = preds.split(\n",
    "                [self.num_classes, 4 * (self.reg_max + 1)], dim=-1\n",
    "            )[0]\n",
    "            \n",
    "            max_cls_scores = torch.max(cls_scores.sigmoid(), dim=1)[0]\n",
    "            return max_cls_scores\n",
    "    \n",
    "wrapper = WrapperModel(cfg, model_path, logger, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e421bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(image_path)\n",
    "\n",
    "raw_height = img.shape[0]\n",
    "raw_width  = img.shape[1]\n",
    "dst_width, dst_height = cfg.data.val.input_size\n",
    "ResizeM = np.eye(3)\n",
    "ResizeM[0, 0] *= dst_width / raw_width\n",
    "ResizeM[1, 1] *= dst_height / raw_height\n",
    "\n",
    "# scaling only\n",
    "numpy_img_warped = cv2.warpPerspective(img, \n",
    "                                       ResizeM, \n",
    "                                       dsize=tuple(cfg.data.val.input_size), \n",
    "                                       flags = cv2.INTER_LINEAR, \n",
    "                                       borderMode = cv2.BORDER_CONSTANT)\n",
    "\n",
    "# normalise\n",
    "mean, std = cfg.data.val.pipeline[\"normalize\"]\n",
    "mean = np.array(mean, dtype=np.float32).reshape(1, 1, 3) / 255\n",
    "std = np.array(std, dtype=np.float32).reshape(1, 1, 3) / 255\n",
    "numpy_img_normalised = ((numpy_img_warped.astype(np.float32) / 255) - mean) / std\n",
    "\n",
    "# numpy to pytorch\n",
    "processed_tensor_img = torch.from_numpy(numpy_img_normalised.transpose(2, 0, 1)).to(device).type(torch.cuda.FloatTensor).unsqueeze(0)\n",
    "\n",
    "# input_   = torch.cat([processed_tensor_img, processed_tensor_img])\n",
    "input_ = processed_tensor_img\n",
    "print(input_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305c4cd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2940]], device='cuda:0', grad_fn=<MaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reedless/.venv/captum-vas/lib/python3.8/site-packages/torch/nn/functional.py:3451: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/home/reedless/.venv/captum-vas/lib/python3.8/site-packages/torch/nn/functional.py:3499: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(wrapper(input_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f213448",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d87669",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reedless/.venv/captum-vas/lib/python3.8/site-packages/torch/nn/functional.py:3451: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/home/reedless/.venv/captum-vas/lib/python3.8/site-packages/torch/nn/functional.py:3499: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "/home/reedless/.venv/captum-vas/lib/python3.8/site-packages/captum/_utils/gradient.py:56: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/reedless/.venv/captum-vas/lib/python3.8/site-packages/captum/attr/_core/deep_lift.py:320: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n",
      "/home/reedless/.venv/captum-vas/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A Module LeakyReLU(negative_slope=0.1, inplace=True) was detected that does not contain some of the input/output attributes that are required for DeepLift computations. This can occur, for example, if your module is being used more than once in the network.Please, ensure that module is being used only once in the network.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ig = IntegratedGradients(wrapper)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# attributions, delta = ig.attribute(input_,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#                                     target=pred_class,\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#                                     return_convergence_delta=True)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m dl \u001b[38;5;241m=\u001b[39m DeepLift(wrapper)\n\u001b[0;32m---> 17\u001b[0m attributions, delta \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/captum-vas/lib/python3.8/site-packages/captum/log/__init__.py:35\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/captum-vas/lib/python3.8/site-packages/captum/attr/_core/deep_lift.py:347\u001b[0m, in \u001b[0;36mDeepLift.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta, custom_attribution_func)\u001b[0m\n\u001b[1;32m    337\u001b[0m expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(\n\u001b[1;32m    338\u001b[0m     target, \u001b[38;5;241m2\u001b[39m, expansion_type\u001b[38;5;241m=\u001b[39mExpansionTypes\u001b[38;5;241m.\u001b[39mrepeat\n\u001b[1;32m    339\u001b[0m )\n\u001b[1;32m    341\u001b[0m wrapped_forward_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_forward_func(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    343\u001b[0m     (inputs, baselines),\n\u001b[1;32m    344\u001b[0m     expanded_target,\n\u001b[1;32m    345\u001b[0m     additional_forward_args,\n\u001b[1;32m    346\u001b[0m )\n\u001b[0;32m--> 347\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_forward_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_attribution_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiplies_by_inputs:\n",
      "File \u001b[0;32m~/.venv/captum-vas/lib/python3.8/site-packages/captum/_utils/gradient.py:118\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "File \u001b[0;32m~/.venv/captum-vas/lib/python3.8/site-packages/torch/autograd/__init__.py:223\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/captum-vas/lib/python3.8/site-packages/captum/attr/_core/deep_lift.py:488\u001b[0m, in \u001b[0;36mDeepLift._backward_hook\u001b[0;34m(self, module, grad_input, grad_output, eps)\u001b[0m\n\u001b[1;32m    486\u001b[0m attr_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msatisfies_attribute_criteria(module)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr_criteria:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA Module \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was detected that does not contain some of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe input/output attributes that are required for DeepLift \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputations. This can occur, for example, if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour module is being used more than once in the network.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease, ensure that module is being used only once in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(module)\n\u001b[1;32m    495\u001b[0m     )\n\u001b[1;32m    496\u001b[0m multipliers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    497\u001b[0m     SUPPORTED_NON_LINEAR[\u001b[38;5;28mtype\u001b[39m(module)](\n\u001b[1;32m    498\u001b[0m         module, module\u001b[38;5;241m.\u001b[39minput, module\u001b[38;5;241m.\u001b[39moutput, grad_input, grad_output, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# remove all the properies that we set for the inputs and output\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A Module LeakyReLU(negative_slope=0.1, inplace=True) was detected that does not contain some of the input/output attributes that are required for DeepLift computations. This can occur, for example, if your module is being used more than once in the network.Please, ensure that module is being used only once in the network."
     ]
    }
   ],
   "source": [
    "from captum.attr import (Deconvolution, DeepLift, DeepLiftShap,\n",
    "                         FeatureAblation, GradientShap, GuidedBackprop,\n",
    "                         GuidedGradCam, InputXGradient, IntegratedGradients,\n",
    "                         Occlusion, Saliency)\n",
    "\n",
    "baseline = torch.zeros(input_.shape).to(device).type(torch.cuda.FloatTensor)\n",
    "\n",
    "thres = 0.35\n",
    "class_scores = wrapper(input_)\n",
    "pred_class = 0\n",
    "\n",
    "# ig = IntegratedGradients(wrapper)\n",
    "# attributions, delta = ig.attribute(input_,\n",
    "#                                     target=pred_class,\n",
    "#                                     return_convergence_delta=True)\n",
    "dl = DeepLift(wrapper)\n",
    "attributions, delta = dl.attribute(input_, baseline, target=pred_class, return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e0d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104db113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fb8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b66748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c3f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum-vas",
   "language": "python",
   "name": "captum-vas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

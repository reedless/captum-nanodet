{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0341257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from nanodet.util import cfg, load_config, Logger\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06be8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'demo/nanodet-plus-m_416.yml'\n",
    "model_path = 'demo/nanodet-plus-m_416_checkpoint.ckpt'\n",
    "image_path = 'demo/000252.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fe18e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'demo/nanodet-plus-m_416.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, use_tensorboard\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/nanodet/nanodet/util/config.py:31\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(cfg, args_cfg)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_config\u001b[39m(cfg, args_cfg):\n\u001b[1;32m     30\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mdefrost()\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m~/nanodet/nanodet/util/yacs.py:207\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[0;34m(self, cfg_filename)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_from_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg_filename):\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a yaml config file and merge it this CfgNode.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    208\u001b[0m         cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_cfg(f)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_from_other_cfg(cfg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'demo/nanodet-plus-m_416.yml'"
     ]
    }
   ],
   "source": [
    "load_config(cfg, config_path)\n",
    "logger = Logger(-1, use_tensorboard=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanodet.model.arch import build_model\n",
    "from nanodet.util import Logger, cfg, load_config, load_model_weight\n",
    "from nanodet.data.transform import Pipeline\n",
    "from nanodet.data.collate import naive_collate\n",
    "from nanodet.data.batch_process import stack_batch_img\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "    def __init__(self, cfg, model_path, logger, device=\"cuda:0\"):\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        model = build_model(cfg.model)\n",
    "        ckpt = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "        load_model_weight(model, ckpt, logger)\n",
    "\n",
    "        self.model = model.to(device).eval()\n",
    "        self.pipeline = Pipeline(cfg.data.val.pipeline, cfg.data.val.keep_ratio)\n",
    "\n",
    "    def inference(self, img):\n",
    "        img_info = {\"id\": 0}\n",
    "        if isinstance(img, str):\n",
    "            img_info[\"file_name\"] = os.path.basename(img)\n",
    "            img = cv2.imread(img)\n",
    "            print(img.shape)\n",
    "        else:\n",
    "            img_info[\"file_name\"] = None\n",
    "        \n",
    "#         if isinstance(img, torch.cuda.FloatTensor):\n",
    "#             img = img.cpu().detach().numpy()[0]\n",
    "#             print(img.shape)\n",
    "\n",
    "        height, width = img.shape[:2]\n",
    "        img_info[\"height\"] = height\n",
    "        img_info[\"width\"] = width\n",
    "        meta = dict(img_info=img_info, raw_img=img, img=img)\n",
    "        \n",
    "        # meta[\"img\"] must be numpy in pipeline\n",
    "        meta = self.pipeline(None, meta, self.cfg.data.val.input_size)\n",
    "        print(meta[\"img\"].shape)\n",
    "        \n",
    "        meta[\"img\"] = torch.from_numpy(meta[\"img\"].transpose(2, 0, 1)).to(self.device).type(torch.cuda.FloatTensor)\n",
    "        print(meta[\"warp_matrix\"])\n",
    "        \n",
    "        \n",
    "        # collate list of meta into a single meta\n",
    "        meta_list = [meta]\n",
    "        meta = naive_collate(meta_list)\n",
    "        \n",
    "        # just stacks the tensors up\n",
    "        meta[\"img\"] = stack_batch_img(meta[\"img\"], divisible=32)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            results = self.model.inference(meta)\n",
    "        return meta, results\n",
    "\n",
    "    def visualize(self, dets, meta, class_names, score_thres, wait=0):\n",
    "        time1 = time.time()\n",
    "        result_img = self.model.head.show_result(\n",
    "            meta[\"raw_img\"][0], dets, class_names, score_thres=score_thres, show=True\n",
    "        )\n",
    "        print(\"viz time: {:.3f}s\".format(time.time() - time1))\n",
    "        return result_img\n",
    "    \n",
    "predictor = Predictor(cfg, model_path, logger, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345940c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg.data.val.pipeline, cfg.data.val.keep_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c3bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data.val.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5d9cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta, res = predictor.inference(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e408a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"img\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916859b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([1, 333, 500, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac275c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(meta[\"img\"]))\n",
    "\n",
    "preds = predictor.model(meta[\"img\"])\n",
    "\n",
    "num_classes = 80\n",
    "reg_max = 7\n",
    "\n",
    "cls_scores, bbox_preds = preds.split(\n",
    "            [num_classes, 4 * (reg_max + 1)], dim=-1\n",
    "        )\n",
    "\n",
    "values, indices = torch.max(cls_scores.sigmoid()[0], dim=0)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4030977785587311\n",
    "0.7854804992675781\n",
    "0.7465116381645203\n",
    "0.3727158010005951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6036f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c93ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0ca48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d119b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "def cv2_imshow(a, convert_bgr_to_rgb=True):\n",
    "    \"\"\"A replacement for cv2.imshow() for use in Jupyter notebooks.\n",
    "    Args:\n",
    "        a: np.ndarray. shape (N, M) or (N, M, 1) is an NxM grayscale image. shape\n",
    "            (N, M, 3) is an NxM BGR color image. shape (N, M, 4) is an NxM BGRA color\n",
    "            image.\n",
    "        convert_bgr_to_rgb: switch to convert BGR to RGB channel.\n",
    "    \"\"\"\n",
    "    a = a.clip(0, 255).astype('uint8')\n",
    "    # cv2 stores colors as BGR; convert to RGB\n",
    "    if convert_bgr_to_rgb and a.ndim == 3:\n",
    "        if a.shape[2] == 4:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "    display(Image.fromarray(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97561e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanodet.util import overlay_bbox_cv\n",
    "\n",
    "result = overlay_bbox_cv(meta['raw_img'][0], res[0], cfg.class_names, score_thresh=0.35)\n",
    "\n",
    "cv2_imshow(cv2.resize(result, None, fx=1.0, fy=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af622aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e061fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for label in res[0]:\n",
    "    for bbox in res[0][label]:\n",
    "        score = bbox[-1]\n",
    "        scores.append(score)\n",
    "        if score > 0.35:\n",
    "            print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba820a",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e31d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wrapper model should take in tensor, return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad50f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModel(torch.nn.Module):\n",
    "    def __init__(self, predictor, device):\n",
    "        super().__init__()\n",
    "        self.model = predictor\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, batched_imgs):\n",
    "        for i in range(len(batched_imgs)):\n",
    "            _, outputs = predictor.inference(batched_imgs[i].cpu().detach().numpy())\n",
    "            acc = []\n",
    "\n",
    "            for j in range(len(outputs)):\n",
    "                sum_scores = torch.zeros((1, len(outputs[j]))).float()\n",
    "\n",
    "                for k in range(len(outputs[j])):\n",
    "                    scores = [box[4] for box in outputs[j][k]]\n",
    "                    sum_scores[0][k] += float(sum(scores))\n",
    "\n",
    "                acc.append(sum_scores)\n",
    "\n",
    "            if len(acc) == 1:\n",
    "                return acc[0]\n",
    "\n",
    "            return torch.stack(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9632e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from captum.attr import (Deconvolution, DeepLift, DeepLiftShap,\n",
    "                         FeatureAblation, GradientShap, GuidedBackprop,\n",
    "                         GuidedGradCam, InputXGradient, IntegratedGradients,\n",
    "                         Occlusion, Saliency)\n",
    "\n",
    "wrapper = WrapperModel(predictor, device)\n",
    "pred_class = 7\n",
    "img = cv2.imread(image_path)\n",
    "input_   = torch.from_numpy(img).permute(2,0,1).unsqueeze(0).to(device).type(torch.cuda.FloatTensor)\n",
    "\n",
    "# Integrated Gradients\n",
    "ig = IntegratedGradients(wrapper)\n",
    "attributions, delta = ig.attribute(input_,\n",
    "                                target=pred_class,\n",
    "                                return_convergence_delta=True)\n",
    "print('Integrated Gradients Convergence Delta:', delta)\n",
    "print(attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff762212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ededf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3223e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91277b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108c4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2161865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9423e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab2c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5cc61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f477c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161717f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8986cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde9bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc708b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d0c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum-vas",
   "language": "python",
   "name": "captum-vas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
